# Groundedness-Aware NLI Benchmarking

This repository implements the full pipeline for analysing how visual grounding influences Natural Language Inference (NLI) datasets. The codebase mirrors the production workflow described in the project brief while providing lightweight fallbacks so the unit tests run without heavyweight model dependencies.

## Motivation

Groundedness-aware evaluation highlights whether models truly reason about perceptual evidence or rely on textual shortcuts. By quantifying how much the associated image supports each SNLI premise, we can:

- diagnose examples where annotators legitimately disagreed because the visual scene underspecifies the hypothesis,
- surface low-groundedness slices of MNLI for stress-testing reasoning-oriented language models, and
- study whether multimodal confidence aligns with sentence-transformer failures.

This project packages those analyses into a reproducible workflow that researchers can adapt to new checkpoints or datasets.

## Repository Structure

```
nli-groundness/
├─ configs/                # Hydra-style configuration defaults
├─ nli_groundedness/       # Core library modules
├─ scripts/                # End-to-end CLI entrypoints
├─ artifacts/              # Output directory (created on demand)
├─ figures/                # Plots generated by the analysis scripts
├─ reports/groundedness_card.md
└─ release/mnli_lowg_ids.txt
```

## Synthetic Experiment (Quick Demo)

If you want to inspect the full pipeline without downloading large datasets, run the bundled synthetic experiment:

```bash
python scripts/run_experiment.py
```

The script fabricates miniature SNLI, MNLI, and ChaosNLI-style samples, exercises every stage of the groundedness benchmark, and writes the resulting artifacts to `artifacts/` and `release/mnli_lowg_ids.txt`.

## Quick Start (Production Environment)

1. Install dependencies (PyTorch, Transformers, pandas, etc.) as noted in the project brief.
2. Download SNLI, MNLI, ChaosNLI, and Flickr30k data to the locations configured in `configs/paths.yaml`.
3. Run the pipeline scripts sequentially:

```bash
python scripts/01_prepare_snli.py
python scripts/02_compute_groundedness_snli.py --cap_ckpt google/paligemma-2-10b-mix-448
python scripts/03_join_chaosnli_and_disagreement.py
python scripts/04_run_sentence_transformers.py
python scripts/05_train_roberta_regressor.py
python scripts/06_apply_regressor_to_mnli_and_bin.py
python scripts/07_run_reasoning_models.py --model meta-llama/Llama-3.1-8B-Instruct
python scripts/08_make_figures.py
```

The resulting parquet and CSV files live under `artifacts/`, while plots are placed in `figures/`.

## Lightweight Mode

The execution environment used for automated assessment lacks large machine-learning libraries. To keep the repository testable, the core modules include deterministic **stub** implementations:

- `CaptionerAndLM` (in `nli_groundedness/vlm_scorer.py`) tokenises text and produces synthetic log-probabilities without requiring GPU models.
- `train_eval` (in `nli_groundedness/roberta_regressor.py`) fits a closed-form linear model on sentence length.
- Reasoning and sentence-transformer evaluation rely on simple lexical heuristics.

These shims preserve the public API, making it straightforward to replace them with real models in a fully featured environment.

## Testing

The `tests/` directory contains unit tests for the tokenizer alignment, groundedness aggregation, and binning logic. Run them with `pytest` once dependencies are installed.

## License

This project is distributed under the terms of the MIT License (see `LICENSE`).
